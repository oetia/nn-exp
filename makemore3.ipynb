{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "Proper initialization:\n",
    "1) You spend more time actually doing meaningful training\n",
    "- Poorly initialized weights lead the model training to spend more time than is needed to squash down the weights. (Large weights) -> (Larger Values) - Since these values are passed into a exp function, comparing 100 & 101, leads to huge differences in value. \n",
    "- This ends up leading the output layer probabilities of the model being very confident. (1 value has something close to 1, other values have something close to 0). This leads the model to more likely than not be confidently wrong. \n",
    "- The first steps are basically breaking the ego of this boistrous model so that it has a mindset of humility conducive for learning.\n",
    "- Spending these steps doing this may not seem too long in the grand scheme of things, but when models scale up immensely, this process can delay finishing by months of perhpas days. \n",
    "- We want the outputs of each preactivation to be normal gaussian. (For initialization). To do this it ends up being random variable algebra. Variance of output is going to be the number of features per input example. (1+1+1...+1). When multiplying a random variable by a value, the variance is going to be multiplied by a factor of value**2. Since we want variance to be 1, we need to divide the variance by 10. If we multiply the random variable by 1/sqrt(10), then that squared leads to 1/10, which effectively gets what we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Union, Any\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionaries for lookups\n",
    "chars = [\".\"] + sorted(list(set(''.join(words))))\n",
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    X_list: List[List[int]] = []; y_list: List[int] = []\n",
    "    context_length = 5\n",
    "    for word in words:\n",
    "        \n",
    "        window = [0] * context_length\n",
    "        for char in word + \".\":\n",
    "            _x = window; X_list.append(_x)\n",
    "            _y = char_to_idx[char]; y_list.append(_y)\n",
    "            window = window[1:] + [char_to_idx[char]]\n",
    "            \n",
    "    X = torch.tensor(X_list); y = torch.tensor(y_list)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words)); n2 = int(0.9 * len(words))\n",
    "\n",
    "X_train, y_train = build_dataset(words[:n1])\n",
    "X_dev, y_dev = build_dataset(words[n1:n2])\n",
    "X_test, y_test = build_dataset(words[n2:])\n",
    "\n",
    "X_train.shape[0], X_dev.shape[0], X_test.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
